<!DOCTYPE HTML>
<html>

<head>
	<link rel="bookmark"  type="image/x-icon"  href="/img/logo_miccall.png"/>
	<link rel="shortcut icon" href="/img/logo_miccall.png">
	
			    <title>
    Hexo
    </title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="/css/mic_main.css" />
    <link rel="stylesheet" href="/css/dropdownMenu.css" />
    <meta name="keywords" content="miccall" />
    
    	<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
	 
    <noscript>
        <link rel="stylesheet" href="/css/noscript.css" />
    </noscript>
    <style type="text/css">
        body:before {
          content: ' ';
          position: fixed;
          top: 0;
          background: url('/img/bg.jpg') center 0 no-repeat;
          right: 0;
          bottom: 0;
          left: 0;
          background-size: cover; 
        }
    </style>

			    
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script async type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


    <script src="/js/jquery.min.js"></script>
    <script src="/js/jquery.scrollex.min.js"></script>
    <script src="/js/jquery.scrolly.min.js"></script>
    <script src="/js/skel.min.js"></script>
    <script src="/js/util.js"></script>
    <script src="/js/main.js"></script>
	
<meta name="generator" content="Hexo 6.3.0"></head>
    
		
<!-- Layouts -->



<!--  代码渲染  -->
<link rel="stylesheet" href="/css/prism_coy.css" />
<link rel="stylesheet" href="/css/typo.css" />
<!-- 文章页 -->
<body class="is-loading">
    <!-- Wrapper 外包 s-->
    <div id="wrapper" class="fade-in">
        <!-- Intro 头部显示 s -->
        <!-- Intro 头部显示 e -->
        <!-- Header 头部logo start -->
        <header id="header">
    <a href="/" class="logo">MICCALL</a>
</header>
        <!-- Nav 导航条 start -->
        <nav id="nav" class="special" >
            <ul class="menu links" >
			<!-- Homepage  主页  --> 
			<li >
	            <a href="/" rel="nofollow">主页</a>
	        </li>
			<!-- categories_name  分类   --> 
	        
	        <li class="active">
	            <a href="#s1">分类</a>
	                    <ul class="submenu">
	                        <li>
	                        
	                    </ul>
	        </li>
	        
	        <!-- archives  归档   --> 
	        
	        
		        <!-- Pages 自定义   -->
		        
		        <li>
		            <a href="/about/" title="个人">
		                个人
		            </a>
		        </li>
		        
		        <li>
		            <a href="/group/" title="团队">
		                团队
		            </a>
		        </li>
		        
		        <li>
		            <a href="/gallery/" title="图库">
		                图库
		            </a>
		        </li>
		        
		        <li>
		            <a href="/tag/" title="标签">
		                标签
		            </a>
		        </li>
		        


            </ul>
            <!-- icons 图标   -->
			<ul class="icons">
                    
                    <li>
                        <a title="github" href="https://github.com/miccall" target="_blank" rel="noopener">
                            <i class="icon fa fa-github"></i>
                        </a>
                    </li>
                    
                    <li>
                        <a title="500px" href="http://500px.com" target="_blank" rel="noopener">
                            <i class="icon fa fa-500px"></i>
                        </a>
                    </li>
                    
			</ul>
</nav>

        <div id="main" >
            <div class ="post_page_title_img" style="height: 25rem;background-image: url(https://qilin-static.s3.cn-northwest-1.amazonaws.com.cn/sandog/aedfh2.jpeg);background-position: center; background-repeat:no-repeat; background-size:cover;-moz-background-size:cover;overflow:hidden;" >
                <a href="#" style="padding: 4rem 4rem 2rem 4rem ;"><h2 >AWS EMR+Dolphin搭建数仓</h2></a>
            </div>
            <!-- Post -->
            <div class="typo" style="padding: 3rem;">
                <h1 id="AWS-EMR-Dolphin搭建数仓"><a href="#AWS-EMR-Dolphin搭建数仓" class="headerlink" title="AWS EMR+Dolphin搭建数仓"></a>AWS EMR+Dolphin搭建数仓</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这个季度的主要工作任务是搭建我司的数仓，DolphinSchedule 调度系统我们之前就一直在用，不过虽然这是个大数据调度系统，我们一直其实没有配套的大数据组件，一直都是使用 python 脚本或者 sql 处理一些离线数据，最终结果也是落在 MySql上 。但是随着业务数据量的扩张，并且我们也希望能将公司别的业务线的数据都接到一个统一的数仓环境中集中处理，因为考虑到我司这边一直用的是 AWS 的云服务，所以就很自然而然的选择了 AWS EMR 构建数仓。</p>
<h2 id="AWS-EMR"><a href="#AWS-EMR" class="headerlink" title="AWS EMR"></a>AWS EMR</h2><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>Amazon EMR（以前称为 Amazon Elastic MapReduce）是一个托管集群平台，可简化在Amazon上运行大数据框架（如 Apache Hadoop 和 Apache Spark）的过程，以处理和分析海量数据。使用这些框架和相关的开源项目，您可以处理用于分析目的的数据和业务情报工作负载。Amazon EMR 还允许您转换大量数据并移出&#x2F;移入到其它Amazon数据存储和数据库中，例如 Amazon Simple Storage Service（Amazon S3）和 Amazon DynamoDB。-摘自官网</p>
<h4 id="AWS-大数据架构设计"><a href="#AWS-大数据架构设计" class="headerlink" title="AWS 大数据架构设计"></a>AWS 大数据架构设计</h4><p>之前公司邀请过 AWS 的技术专家和架构师给我们大概讲述了一下 AWS 对于大数据领域整体的一个架构设计，大概是如图所示，当然，这么简陋的图，肯定不是人家的原图。<br><img src="https://qilin-static.s3.cn-northwest-1.amazonaws.com.cn/sandog/aws-bigdata.jpeg" alt="aws-bigdata"></p>
<ul>
<li>rds：AWS 的数据库</li>
<li>s3：AWS 的对象存储系统</li>
<li>glue：AWS 大数据系统的元数据存储系统</li>
<li>RedShift： 是一种完全托管的 PB 级 AWS 云 中数据仓库服务</li>
</ul>
<p>从AWS的架构设计来说，存储和计算是完全解耦的，EMR更像是个用完即扔掉的工具，只保留计算的结果落在S3上，而如果需要高性能的访问S3上的数据，就可以再购买 RedShift 服务，不管是用 RedShift 直接查询S3，还是将S3的数据导入到 RedShift 都可以。而glue可以理解为存储 hive、RedShift 相关元数据的系统，不管购买的EMR服务是否被回收，存在glue里的元数据都不会被回收。</p>
<h2 id="数仓架构"><a href="#数仓架构" class="headerlink" title="数仓架构"></a>数仓架构</h2><p>这里我们并没有用上述AWS的所谓最佳实践去搭建数仓，一来因为开发较为复杂，需要调研的东西较多，二来成本也比较高，据说RedShift比较贵。</p>
<p><img src="https://qilin-static.s3.cn-northwest-1.amazonaws.com.cn/sandog/aws-emr-2.jpeg" alt="aws-bigdata2"></p>
<p>最终我们的整体架构大概如图所示，分别通过 Sqoop 和 DataX 从 MySQL 和 MongoDB 抽取数据表到 ods  层，数据的清洗、分析分别是通过使用 spark 和 hive sql，最终数据落到 dm层，应用系统使用 presto 查询落在数仓的数据。</p>
<h2 id="DolphinSchedule-集成大数据组件"><a href="#DolphinSchedule-集成大数据组件" class="headerlink" title="DolphinSchedule 集成大数据组件"></a>DolphinSchedule 集成大数据组件</h2><p>DolphinSchedule 本身只是个调度系统，本身并不包含如 Hadoop、Hive、DataX、Sqoop 这些组件，如果想通过 DolphinSchedule 调用这些大数据组件，还是得手动将这些组件安装到 Dolphin 部署的机器上。</p>
<h4 id="远程操作Hadoop"><a href="#远程操作Hadoop" class="headerlink" title="远程操作Hadoop"></a>远程操作Hadoop</h4><p>因为我们的数据是通过 HDFS 落在 S3上的，当然也有一部分数据是直接落在 EMR集群里的 HDFS 上，所以远程操作 Hadoop 的本质就是远程修改 HDFS 上的文件。这里我们可以先看一下 我们EMR集群上的Hadoop 、Hive、Sqoop、Spark 的版本，从官网上下载一个相同版本的到部署了DolphinSchedule Worker节点的服务器上。</p>
<p>这里将组件下载到指定服务器后，就需要改一些关键性的配置了。</p>
<ul>
<li>Hadoop 的 core-site.xml</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;!-- 这里我们操作的是远程hdfs，所以要把地址换成 emr master那台服务器--&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://xxx:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;ipc.client.connect.max.retries.on.timeouts&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;5&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.security.key.default.bitlength&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;256&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;property&gt;</span><br><span class="line">  &lt;!-- s3 地址，如果需要数据落在s3上的话就必须得指明s3地址--&gt;</span><br><span class="line">    &lt;name&gt;fs.s3n.endpoint&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;s3.cn-xxxx&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">   &lt;!-- 这里指明s3的驱动--&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.s3.impl&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;fs.s3n.impl&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;com.amazon.ws.emr.hadoop.fs.EmrFileSystem&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.s3.buffer.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/mnt/s3&lt;/value&gt;</span><br><span class="line">    &lt;final&gt;true&lt;/final&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>hive 的 hive-site.xml</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 同样是要修改hdfs地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://xxxhdfs:8020&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"></span><br><span class="line"> &lt;!-- 元数据配置--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.metastore.uris&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;thrift://xxx:9083&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;jdbc:mysql://xxx:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;org.mariadb.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;xxx&lt;/value&gt;</span><br><span class="line">    &lt;description&gt;password to use against metastore database&lt;/description&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line"> &lt;!-- 开启自动提交，不然会本地执行，我们希望能提交到yarn里面去执行--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.mode.local.auto&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>sqoop 和 spark 需要设置好 sqoop-env.sh 和 spark-env.sh 就差不多ok了</li>
</ul>
<p>最后试试 <code>hadoop dfs -ls /</code> 之类的命令是否能达到相应效果。</p>
<h4 id="将组件集成进-DolphinSchedule"><a href="#将组件集成进-DolphinSchedule" class="headerlink" title="将组件集成进 DolphinSchedule"></a>将组件集成进 DolphinSchedule</h4><p>DolphinSchedule 在执行任何任务节点之前都会执行这一句命令行 <code>source /opt/dolphinscheduler/conf/dolphinscheduler_env.sh</code>，其实就是在指定环境变量，所以我们修改 这个文件就OK了</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># JAVA_HOME, will use it to start DolphinScheduler server</span><br><span class="line">export JAVA_HOME=$&#123;JAVA_HOME:-/opt/soft/java&#125;</span><br><span class="line"></span><br><span class="line"># Database related configuration, set database type, username and password</span><br><span class="line">export DATABASE=$&#123;DATABASE:-postgresql&#125;</span><br><span class="line">export SPRING_PROFILES_ACTIVE=$&#123;DATABASE&#125;</span><br><span class="line">export SPRING_DATASOURCE_URL</span><br><span class="line">export SPRING_DATASOURCE_USERNAME</span><br><span class="line">export SPRING_DATASOURCE_PASSWORD</span><br><span class="line"></span><br><span class="line"># DolphinScheduler server related configuration</span><br><span class="line">export SPRING_CACHE_TYPE=$&#123;SPRING_CACHE_TYPE:-none&#125;</span><br><span class="line">export SPRING_JACKSON_TIME_ZONE=$&#123;SPRING_JACKSON_TIME_ZONE:-UTC&#125;</span><br><span class="line">export MASTER_FETCH_COMMAND_NUM=$&#123;MASTER_FETCH_COMMAND_NUM:-10&#125;</span><br><span class="line"></span><br><span class="line"># Registry center configuration, determines the type and link of the registry center</span><br><span class="line">export REGISTRY_TYPE=$&#123;REGISTRY_TYPE:-zookeeper&#125;</span><br><span class="line">export REGISTRY_ZOOKEEPER_CONNECT_STRING=$&#123;REGISTRY_ZOOKEEPER_CONNECT_STRING:-localhost:2181&#125;</span><br><span class="line"></span><br><span class="line"># 将 Hadoop 、 Hive 、SQOOP、Spark 指定为我们上传到服务器上的地址</span><br><span class="line">export HADOOP_HOME=$&#123;HADOOP_HOME:-/opt/soft/hadoop/hadoop-3.2.1&#125;</span><br><span class="line">export HADOOP_CONF_DIR=$&#123;HADOOP_CONF_DIR:-/opt/soft/hadoop/hadoop-3.2.1/etc/hadoop&#125;</span><br><span class="line">export SPARK_HOME1=$&#123;SPARK_HOME1:-/opt/soft/hadoop/spark-3.0.1&#125;</span><br><span class="line">export SPARK_HOME2=$&#123;SPARK_HOME2:-/opt/soft/hadoop/spark-3.0.1&#125;</span><br><span class="line">export PYTHON_HOME=$&#123;PYTHON_HOME:-/opt/soft/python&#125;</span><br><span class="line">export HIVE_HOME=$&#123;HIVE_HOME:-/opt/soft/hadoop/hive-3.1.2&#125;</span><br><span class="line">export FLINK_HOME=$&#123;FLINK_HOME:-/opt/soft/flink&#125;</span><br><span class="line">export DATAX_HOME=$&#123;DATAX_HOME:-/opt/soft/datax&#125;</span><br><span class="line"></span><br><span class="line">export SQOOP_HOME=/opt/soft/hadoop/sqoop-1.4.7</span><br><span class="line"></span><br><span class="line">export PATH=$HADOOP_HOME/bin:$SPARK_HOME1/bin:$SPARK_HOME2/bin:$PYTHON_HOME/bin:$JAVA_HOME/bin:$HIVE_HOME/bin:$FLINK_HOME/bin:$DATAX_HOME/bin:$SQOOP_HOME/bin:$PATH</span><br></pre></td></tr></table></figure>

<p>然后再去新建一个 Sqoop 工作流试试能不能正常运转</p>
<h2 id="MySQL数据抽取"><a href="#MySQL数据抽取" class="headerlink" title="MySQL数据抽取"></a>MySQL数据抽取</h2><h3 id="生成Hive表"><a href="#生成Hive表" class="headerlink" title="生成Hive表"></a>生成Hive表</h3><h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><p>这里我们首先建立一张表，表示对于 需要抽取表的定义</p>
<table>
<thead>
<tr>
<th>字段</th>
<th>备注</th>
</tr>
</thead>
<tbody><tr>
<td>source_schema</td>
<td>源表的 schema，比如 zaihui_shennong</td>
</tr>
<tr>
<td>source_schema_alias</td>
<td>schema 的简写，比如 zaihui_shennong 简写为 shennong</td>
</tr>
<tr>
<td>source_table</td>
<td>源表的表名，比如 sn_biz_shop</td>
</tr>
<tr>
<td>is_incremental</td>
<td>是否增量抽取，1 为增量抽取，0为全量抽取，200百万以下的表就直接增量抽取就ok了</td>
</tr>
<tr>
<td>incremental_columns</td>
<td>增量抽取使用的字段，一般使用 update_time</td>
</tr>
<tr>
<td>id_str</td>
<td>表的主键id是否为字符串格式</td>
</tr>
</tbody></table>
<p>这里我们只定义了表名，并不会列举所有字段，因为字段可以靠 sqoop create-hive-table 功能去搞定，然后我们需要两个脚本，一个脚本是每次新增一个需要抽取的 MySQL表时，需要执行一遍的脚本，这个脚本用来创建对应的Hive表，另一个脚本是生成DolphinSchedule工作流的脚本，将新加入的这张表添加进MySQL抽取的工作流。</p>
<h4 id="生成-Hive-表脚本实现"><a href="#生成-Hive-表脚本实现" class="headerlink" title="生成 Hive 表脚本实现"></a>生成 Hive 表脚本实现</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import os</span><br><span class="line">import random</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MYSQL_HOST = &quot;&quot;</span><br><span class="line">MYSQL_PORT = 3306</span><br><span class="line">MYSQL_USER = &quot;&quot;</span><br><span class="line">MYSQL_PASSWD = &quot;&quot;</span><br><span class="line"></span><br><span class="line">S3_ROOT = &quot;s3://...&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MysqlCdhMeta:</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        source_schema: str,</span><br><span class="line">        source_schema_alias: str,</span><br><span class="line">        source_table: str,</span><br><span class="line">        is_incremental: int,</span><br><span class="line">        incremental_columns: str,</span><br><span class="line">        id_str: int,</span><br><span class="line">    ):</span><br><span class="line">        self.source_schema = source_schema</span><br><span class="line">        self.source_schema_alias = source_schema_alias</span><br><span class="line">        self.source_table = source_table</span><br><span class="line">        self.is_incremental = is_incremental</span><br><span class="line">        self.incremental_columns = incremental_columns</span><br><span class="line">        self.id_str = id_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_from_mysql_meta(query_sql: str):</span><br><span class="line">    conn = pymysql.connect(</span><br><span class="line">        host=MYSQL_HOST,</span><br><span class="line">        port=MYSQL_PORT,</span><br><span class="line">        user=MYSQL_USER,</span><br><span class="line">        passwd=MYSQL_PASSWD,</span><br><span class="line">        db=&quot;&quot;,</span><br><span class="line">        charset=&quot;utf8&quot;,</span><br><span class="line">    )</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">    cursor.execute(query_sql)</span><br><span class="line">    result = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    conn.close()</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_from_hive_meta(query_sql: str):</span><br><span class="line">    conn = pymysql.connect(</span><br><span class="line">        host=&quot;&quot;,</span><br><span class="line">        port=3306,</span><br><span class="line">        user=&quot;&quot;,</span><br><span class="line">        passwd=&quot;&quot;,</span><br><span class="line">        db=&quot;&quot;,</span><br><span class="line">        charset=&quot;utf8&quot;,</span><br><span class="line">    )</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">    cursor.execute(query_sql)</span><br><span class="line">    result = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    conn.close()</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def exec_from_hive(query_sql: str):</span><br><span class="line">    file_name = f&quot;/tmp/hive_tmp/&#123;time.time()&#125;_&#123;random.randint(0, 1000)&#125;.hql&quot;</span><br><span class="line">    tmp_file = open(file_name, &quot;w&quot;)</span><br><span class="line">    tmp_file.write(query_sql)</span><br><span class="line">    tmp_file.close()</span><br><span class="line">    os.system(f&quot;hive -f &#123;file_name&#125;&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_all_mysql_cdh_meta():</span><br><span class="line">    mysql_meta_db_data = query_from_mysql_meta(</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        select </span><br><span class="line">            source_schema, source_schema_alias, source_table, is_incremental, incremental_columns, id_str</span><br><span class="line">        from mysql_cdh_meta;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    )</span><br><span class="line">    if mysql_meta_db_data is None or len(mysql_meta_db_data) == 0:</span><br><span class="line">        return []</span><br><span class="line">    return [</span><br><span class="line">        MysqlCdhMeta(</span><br><span class="line">            source_schema=db_data[0],</span><br><span class="line">            source_schema_alias=db_data[1],</span><br><span class="line">            source_table=db_data[2],</span><br><span class="line">            is_incremental=db_data[3],</span><br><span class="line">            incremental_columns=db_data[4],</span><br><span class="line">            id_str=db_data[5],</span><br><span class="line">        )</span><br><span class="line">        for db_data in mysql_meta_db_data</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_table_name(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    return f&quot;&#123;mysql_cdh_meta.source_schema_alias&#125;_&#123;mysql_cdh_meta.source_table&#125;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查之前是否创建过这张hive表</span><br><span class="line">def check_hive_table_create(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    hive_db_data = query_from_hive_meta(</span><br><span class="line">        f&quot;&quot;&quot;</span><br><span class="line">        select t.* from TBLS t</span><br><span class="line">        inner join DBS d</span><br><span class="line">        on t.DB_ID = d.DB_ID</span><br><span class="line">        where d.NAME = &#x27;stg&#x27;</span><br><span class="line">          and t.TBL_NAME = &#x27;&#123;get_hive_table_name(mysql_cdh_meta)&#125;&#x27;</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    )</span><br><span class="line">    if hive_db_data is None or len(hive_db_data) == 0:</span><br><span class="line">        return False</span><br><span class="line">    return True</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_column(hive_database: str, hive_table: str):</span><br><span class="line">    # 如果不指定写死 columns 的顺序，则有可能会乱</span><br><span class="line">    # 所以从hive的元数据中查询出hive表的字段，并根据INTEGER_IDX 排序，这样字段就不会乱</span><br><span class="line">    hive_columns = query_from_hive_meta(</span><br><span class="line">        f&quot;&quot;&quot;</span><br><span class="line">        select col.COLUMN_NAME</span><br><span class="line">        from COLUMNS_V2 col</span><br><span class="line">        inner join TBLS tbl</span><br><span class="line">        on col.CD_ID = tbl.TBL_ID</span><br><span class="line">        inner join DBS d</span><br><span class="line">        on tbl.DB_ID = d.DB_ID</span><br><span class="line">        where tbl.TBL_NAME = &#x27;&#123;hive_table&#125;&#x27;</span><br><span class="line">        and d.NAME = &#x27;&#123;hive_database&#125;&#x27;</span><br><span class="line">        order by INTEGER_IDX</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    )</span><br><span class="line">    if hive_columns is None or len(hive_columns) == 0:</span><br><span class="line">        return None</span><br><span class="line">    return &quot;,&quot;.join([col[0] for col in hive_columns])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sqoop_command_import_all_data(mysql_cdh_meta: MysqlCdhMeta, hive_database: str):</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    hive_columns = get_hive_column(hive_database, hive_table_name)</span><br><span class="line">    import_data_commands = [</span><br><span class="line">        &quot;sqoop import&quot;,</span><br><span class="line">        # 如果主键id是 字符串型的话，得加上 -Dorg.apache.sqoop.splitter.allow_text_splitter=true</span><br><span class="line">        &quot; -Dorg.apache.sqoop.splitter.allow_text_splitter=true &quot;</span><br><span class="line">        if mysql_cdh_meta.id_str == 1</span><br><span class="line">        else &quot;&quot;,</span><br><span class="line">        f&quot; --connect jdbc:mysql://&#123;MYSQL_HOST&#125;:&#123;MYSQL_PORT&#125;/&#123;mysql_cdh_meta.source_schema&#125;&quot;,</span><br><span class="line">        f&quot; --username &#123;MYSQL_USER&#125; --password &#123;MYSQL_PASSWD&#125;&quot;,</span><br><span class="line">        f&quot; --table &#123;mysql_cdh_meta.source_table&#125;&quot;,</span><br><span class="line">        # 这是Sqoop 导入的格式</span><br><span class="line">        r&quot; --fields-terminated-by &#x27;\001&#x27;&quot;,</span><br><span class="line">        f&quot; --columns &#x27;&#123;hive_columns&#125;&#x27;&quot; if hive_columns is not None else &quot;&quot;,</span><br><span class="line">        &quot;  --delete-target-dir  --compress  --hive-delims-replacement &#x27; &#x27; &quot;,</span><br><span class="line">        f&quot; --target-dir &#123;S3_ROOT&#125;/&#123;hive_database&#125;/&#123;hive_table_name&#125;&quot;,</span><br><span class="line">    ]</span><br><span class="line">    os.system(&quot; &quot;.join(import_data_commands))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sqoop_command_create_hive_table(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    create_table_commands = [</span><br><span class="line">        # sqoop create-hive-table 会自动获取MySQL的表结构，直接映射生成Hive表</span><br><span class="line">        &quot;sqoop create-hive-table&quot;,</span><br><span class="line">        &quot; -Dorg.apache.sqoop.splitter.allow_text_splitter=true &quot;</span><br><span class="line">        if mysql_cdh_meta.id_str == 1</span><br><span class="line">        else &quot;&quot;,</span><br><span class="line">        f&quot; --connect jdbc:mysql://&#123;MYSQL_HOST&#125;:&#123;MYSQL_PORT&#125;/&#123;mysql_cdh_meta.source_schema&#125;&quot;,</span><br><span class="line">        f&quot; --username &#123;MYSQL_USER&#125; --password &#123;MYSQL_PASSWD&#125;&quot;,</span><br><span class="line">        &quot;  --hive-database stg&quot;,</span><br><span class="line">        f&quot; --table &#123;mysql_cdh_meta.source_table&#125;&quot;,</span><br><span class="line">        f&quot; --hive-table &#123;get_hive_table_name(mysql_cdh_meta)&#125;&quot;,</span><br><span class="line">    ]</span><br><span class="line">    os.system(&quot; &quot;.join(create_table_commands))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def create_hive_table(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    sqoop_command_create_hive_table(mysql_cdh_meta)</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    create_hive_sqls = [</span><br><span class="line">        # 建表记得建成 external 表，否则表drop掉，数据也没了</span><br><span class="line">        f&quot;&quot;&quot;create external table if not exists ods.&#123;hive_table_name&#125; </span><br><span class="line">            like stg.&#123;hive_table_name&#125; </span><br><span class="line">            location &#x27;&#123;S3_ROOT&#125;/ods/&#123;hive_table_name&#125;&#x27;;</span><br><span class="line">        &quot;&quot;&quot;,</span><br><span class="line">        f&quot;&quot;&quot;alter table stg.&#123;hive_table_name&#125; </span><br><span class="line">           set location &#x27;&#123;S3_ROOT&#125;/stg/&#123;hive_table_name&#125;&#x27;;</span><br><span class="line">           &quot;&quot;&quot;,</span><br><span class="line">    ]</span><br><span class="line">    exec_from_hive(&quot;&quot;.join(create_hive_sqls))</span><br><span class="line">    sqoop_command_import_all_data(mysql_cdh_meta, &quot;ods&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    all_cdh_meta = get_all_mysql_cdh_meta()</span><br><span class="line">    if len(all_cdh_meta) &gt; 0:</span><br><span class="line">        for cdh_meta in all_cdh_meta:</span><br><span class="line">            if not check_hive_table_create(cdh_meta):</span><br><span class="line">                create_hive_table(cdh_meta)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="生成DolphinSchedule-任务流实现"><a href="#生成DolphinSchedule-任务流实现" class="headerlink" title="生成DolphinSchedule 任务流实现"></a>生成DolphinSchedule 任务流实现</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br></pre></td><td class="code"><pre><span class="line"># coding=utf-8</span><br><span class="line">import pymysql</span><br><span class="line"></span><br><span class="line">from pydolphinscheduler.core import ProcessDefinition</span><br><span class="line">from pydolphinscheduler.tasks import Sql, Shell</span><br><span class="line">from pydolphinscheduler.tasks.sql import SqlType</span><br><span class="line"></span><br><span class="line">MYSQL_HOST = &quot;&quot;</span><br><span class="line">MYSQL_PORT = 3306</span><br><span class="line">MYSQL_USER = &quot;&quot;</span><br><span class="line">MYSQL_PASSWD = &quot;&quot;</span><br><span class="line"></span><br><span class="line">S3_ROOT = &quot;s3://...&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MysqlCdhMeta:</span><br><span class="line">    def __init__(</span><br><span class="line">        self,</span><br><span class="line">        source_schema: str,</span><br><span class="line">        source_schema_alias: str,</span><br><span class="line">        source_table: str,</span><br><span class="line">        is_incremental: int,</span><br><span class="line">        incremental_columns: str,</span><br><span class="line">        id_str: int,</span><br><span class="line">    ):</span><br><span class="line">        self.source_schema = source_schema</span><br><span class="line">        self.source_schema_alias = source_schema_alias</span><br><span class="line">        self.source_table = source_table</span><br><span class="line">        self.is_incremental = is_incremental</span><br><span class="line">        self.incremental_columns = incremental_columns</span><br><span class="line">        self.id_str = id_str</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_table_name(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    return f&quot;&#123;mysql_cdh_meta.source_schema_alias&#125;_&#123;mysql_cdh_meta.source_table&#125;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_from_mysql_meta(query_sql: str):</span><br><span class="line">    conn = pymysql.connect(</span><br><span class="line">        host=MYSQL_HOST,</span><br><span class="line">        port=MYSQL_PORT,</span><br><span class="line">        user=MYSQL_USER,</span><br><span class="line">        passwd=MYSQL_PASSWD,</span><br><span class="line">        db=&quot;hadoop_meta&quot;,</span><br><span class="line">        charset=&quot;utf8&quot;,</span><br><span class="line">    )</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">    cursor.execute(query_sql)</span><br><span class="line">    result = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    conn.close()</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def query_from_hive_meta(query_sql: str):</span><br><span class="line">    conn = pymysql.connect(</span><br><span class="line">        host=&quot;&quot;,</span><br><span class="line">        port=3306,</span><br><span class="line">        user=&quot;hive&quot;,</span><br><span class="line">        passwd=&quot;&quot;,</span><br><span class="line">        db=&quot;&quot;,</span><br><span class="line">        charset=&quot;utf8&quot;,</span><br><span class="line">    )</span><br><span class="line">    cursor = conn.cursor()</span><br><span class="line">    cursor.execute(query_sql)</span><br><span class="line">    result = cursor.fetchall()</span><br><span class="line">    cursor.close()</span><br><span class="line">    conn.close()</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_column(hive_database: str, hive_table: str):</span><br><span class="line">    hive_columns = query_from_hive_meta(</span><br><span class="line">        f&quot;&quot;&quot;</span><br><span class="line">        select col.COLUMN_NAME</span><br><span class="line">        from COLUMNS_V2 col</span><br><span class="line">        inner join TBLS tbl</span><br><span class="line">        on col.CD_ID = tbl.TBL_ID</span><br><span class="line">        inner join DBS d</span><br><span class="line">        on tbl.DB_ID = d.DB_ID</span><br><span class="line">        where tbl.TBL_NAME = &#x27;&#123;hive_table&#125;&#x27;</span><br><span class="line">        and d.NAME = &#x27;&#123;hive_database&#125;&#x27;</span><br><span class="line">        order by INTEGER_IDX</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    )</span><br><span class="line">    if hive_columns is None or len(hive_columns) == 0:</span><br><span class="line">        return None</span><br><span class="line">    return &quot;,&quot;.join([col[0] for col in hive_columns])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_all_mysql_cdh_meta():</span><br><span class="line">    mysql_meta_db_data = query_from_mysql_meta(</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        select </span><br><span class="line">            source_schema, source_schema_alias, source_table, is_incremental, incremental_columns, id_str</span><br><span class="line">        from mysql_cdh_meta order by is_incremental;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">    )</span><br><span class="line">    if mysql_meta_db_data is None or len(mysql_meta_db_data) == 0:</span><br><span class="line">        return []</span><br><span class="line">    return [</span><br><span class="line">        MysqlCdhMeta(</span><br><span class="line">            source_schema=db_data[0],</span><br><span class="line">            source_schema_alias=db_data[1],</span><br><span class="line">            source_table=db_data[2],</span><br><span class="line">            is_incremental=db_data[3],</span><br><span class="line">            incremental_columns=db_data[4],</span><br><span class="line">            id_str=db_data[5],</span><br><span class="line">        )</span><br><span class="line">        for db_data in mysql_meta_db_data</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def hdfs_command_overwrite_his_table(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    distcp_command = f&quot;hadoop distcp -overwrite &#123;S3_ROOT&#125;/ods/&#123;hive_table_name&#125;/ &#123;S3_ROOT&#125;/ods_his/&#123;hive_table_name&#125;/&quot;</span><br><span class="line">    return f&quot;&quot;&quot;&#123;clear_his_table_command(mysql_cdh_meta)&#125;\n&#123;distcp_command&#125;&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def clear_his_table_command(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    return f&quot;hadoop fs -rm -r -f -skipTrash &#123;S3_ROOT&#125;/ods_his/&#123;hive_table_name&#125;/*&quot;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sqoop_command(mysql_cdh_meta: MysqlCdhMeta, hive_database: str):</span><br><span class="line">    where_sql = None</span><br><span class="line">    if mysql_cdh_meta.is_incremental == 1:</span><br><span class="line">        incr_columns = mysql_cdh_meta.incremental_columns.split(&quot;,&quot;)</span><br><span class="line">        where_sql = &quot; or &quot;.join(</span><br><span class="line">            [</span><br><span class="line">                f&quot; &#123;incr_column&#125; &gt;= date_sub(current_date(), interval 1 day) &quot;</span><br><span class="line">                for incr_column in incr_columns</span><br><span class="line">            ]</span><br><span class="line">        )</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    hive_columns = get_hive_column(hive_database, hive_table_name)</span><br><span class="line">    import_data_commands = [</span><br><span class="line">        &quot;sqoop import&quot;,</span><br><span class="line">        &quot; -Dorg.apache.sqoop.splitter.allow_text_splitter=true &quot;</span><br><span class="line">        if mysql_cdh_meta.id_str == 1</span><br><span class="line">        else &quot;&quot;,</span><br><span class="line">        f&quot; --connect jdbc:mysql://&#123;MYSQL_HOST&#125;:&#123;MYSQL_PORT&#125;/&#123;mysql_cdh_meta.source_schema&#125;?useSSL=false&quot;,</span><br><span class="line">        f&quot; --username &#123;MYSQL_USER&#125; --password &#123;MYSQL_PASSWD&#125;&quot;,</span><br><span class="line">        f&quot; --table &#123;mysql_cdh_meta.source_table&#125;&quot;,</span><br><span class="line">        r&quot; --fields-terminated-by &#x27;\001&#x27;&quot;,</span><br><span class="line">        f&quot; --columns &#x27;&#123;hive_columns&#125;&#x27;&quot; if hive_columns is not None else &quot;&quot;,</span><br><span class="line">        &quot;  --delete-target-dir  --compress  --hive-delims-replacement &#x27; &#x27; &quot;,</span><br><span class="line">        f&quot; --where &#x27;&#123;where_sql&#125;&#x27; &quot; if where_sql is not None else &quot;&quot;,</span><br><span class="line">        f&quot; --target-dir &#123;S3_ROOT&#125;/&#123;hive_database&#125;/&#123;get_hive_table_name(mysql_cdh_meta)&#125;&quot;,</span><br><span class="line">    ]</span><br><span class="line">    return &quot; &quot;.join(import_data_commands)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 因为当前 pyds 不支持 sqoop，只能通过这种方式调用 sqoop</span><br><span class="line">def get_sqoop_task(mysql_cdh_meta: MysqlCdhMeta, sqoop_command: str):</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    return Shell(name=f&quot;&#123;hive_table_name&#125;抽取&quot;, command=sqoop_command)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_hive_task(mysql_cdh_meta: MysqlCdhMeta):</span><br><span class="line">    hive_table_name = get_hive_table_name(mysql_cdh_meta)</span><br><span class="line">    hql = f&quot;&quot;&quot;</span><br><span class="line">        insert overwrite table ods.&#123;hive_table_name&#125;</span><br><span class="line">        select t.*</span><br><span class="line">        from ods_his.&#123;hive_table_name&#125; t </span><br><span class="line">        left join stg.&#123;hive_table_name&#125; s</span><br><span class="line">            on t.id=s.id</span><br><span class="line">            where s.id is null</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return Sql(</span><br><span class="line">        name=f&quot;&#123;hive_table_name&#125;增量数据处理&quot;,</span><br><span class="line">        datasource_name=&quot;zaihui_hive&quot;,</span><br><span class="line">        pre_statements=[</span><br><span class="line">            f&quot;&quot;&quot;create table if not exists ods_his.&#123;hive_table_name&#125; </span><br><span class="line">            like ods.&#123;hive_table_name&#125;</span><br><span class="line">            location &#x27;&#123;S3_ROOT&#125;/ods_his/&#123;hive_table_name&#125;&#x27;</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        ],</span><br><span class="line">        post_statements=[</span><br><span class="line">            f&quot;&quot;&quot;</span><br><span class="line">            insert into table ods.&#123;hive_table_name&#125;</span><br><span class="line">            select * from stg.&#123;hive_table_name&#125;</span><br><span class="line">            &quot;&quot;&quot;</span><br><span class="line">        ],</span><br><span class="line">        sql_type=SqlType.NOT_SELECT,</span><br><span class="line">        sql=hql,</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">with ProcessDefinition(</span><br><span class="line">    user=&quot;-&quot;,</span><br><span class="line">    project=&quot;-&quot;,</span><br><span class="line">    name=&quot;-&quot;,</span><br><span class="line">    schedule=&quot;0 1 1 * * ? *&quot;,</span><br><span class="line">    start_time=&quot;2021-01-01&quot;,</span><br><span class="line">    end_time=&quot;2030-01-01&quot;,</span><br><span class="line">    tenant=&quot;root&quot;,</span><br><span class="line">) as pd:</span><br><span class="line">    all_cdh_meta = get_all_mysql_cdh_meta()</span><br><span class="line">    if len(all_cdh_meta) &gt; 0:</span><br><span class="line">        task_array = []</span><br><span class="line">        for cdh_meta in all_cdh_meta:</span><br><span class="line">            print(</span><br><span class="line">                f&quot;---------- start &#123;cdh_meta.source_schema&#125;.&#123;cdh_meta.source_table&#125; ----------&quot;</span><br><span class="line">            )</span><br><span class="line">            if cdh_meta.is_incremental == 0:</span><br><span class="line">                # 全量更新</span><br><span class="line">                task_array.append(</span><br><span class="line">                    get_sqoop_task(cdh_meta, sqoop_command(cdh_meta, &quot;ods&quot;))</span><br><span class="line">                )</span><br><span class="line">            else:</span><br><span class="line">                # 增量更新</span><br><span class="line">                task_array.append(</span><br><span class="line">                    get_sqoop_task(</span><br><span class="line">                        cdh_meta,</span><br><span class="line">                        f&quot;&quot;&quot;&#123;hdfs_command_overwrite_his_table(cdh_meta)&#125;\n&#123;sqoop_command(cdh_meta, &quot;stg&quot;)&#125;&quot;&quot;&quot;,</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line">                task_array.append(get_hive_task(cdh_meta))</span><br><span class="line">                task_array.append(</span><br><span class="line">                    Shell(</span><br><span class="line">                        name=f&quot;&#123;get_hive_table_name(cdh_meta)&#125;_his表清理&quot;,</span><br><span class="line">                        command=clear_his_table_command(cdh_meta),</span><br><span class="line">                    )</span><br><span class="line">                )</span><br><span class="line">        for index in range(len(task_array)):</span><br><span class="line">            if index &gt; 0:</span><br><span class="line">                task_array[index - 1] &gt;&gt; task_array[index]</span><br><span class="line">        pd.submit()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最终生成的 任务流就像这样<br><img src="https://qilin-static.s3.cn-northwest-1.amazonaws.com.cn/sandog/WechatIMG271.jpeg" alt="aws-bigdata2"><br> 串行的，也可以改造成适当并行，提升执行速度</p>

            </div>

            <!-- Post Comments -->
            
    <!-- 使用 DISQUS_CLICK -->
<div id="disqus-comment">
    <div id="disqus_thread"></div>

<!-- add animation -->
<style>
	.disqus_click_btn {
            line-height: 30px;
            margin: 0;
            min-width: 50px;
            padding: 0 14px;
            display: inline-block;
            font-family: "Roboto", "Helvetica", "Arial", sans-serif;
            font-size: 14px;
            font-weight: 400;
            text-transform: uppercase;
            letter-spacing: 0;
            overflow: hidden;
            will-change: box-shadow;
            transition: box-shadow .2s cubic-bezier(.4, 0, 1, 1), background-color .2s cubic-bezier(.4, 0, .2, 1), color .2s cubic-bezier(.4, 0, .2, 1);
            outline: 0;
            cursor: pointer;
            text-decoration: none;
            text-align: center;
            vertical-align: middle;
            border: 0;
            background: rgba(158, 158, 158, .2);
            box-shadow: 0 2px 2px 0 rgba(0, 0, 0, .14), 0 3px 1px -2px rgba(0, 0, 0, .2), 0 1px 5px 0 rgba(0, 0, 0, .12);
            color: #fff;
            background-color: #7EC0EE;
            text-shadow: 0
        }
</style>
	
<div class="btn_click_load" id="disqus_bt"> 
    <button class="disqus_click_btn">点击查看评论</button>
</div>

<!--
<script type="text/javascript">
$('.btn_click_load').click(function() {
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'http-miccall-tech'; // required: replace example with your forum shortname

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();

    document.getElementById('disqus_bt').style.display = "none";
});
</script>
-->
<script type="text/javascript">
    var disqus_config = function () {
        this.page.url = 'http://example.com/2022/12/12/AWS%20EMR+Dolphin%E6%90%AD%E5%BB%BA%E6%95%B0%E4%BB%93/';  // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://example.com/2022/12/12/AWS%20EMR+Dolphin%E6%90%AD%E5%BB%BA%E6%95%B0%E4%BB%93/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
</script>

<script type="text/javascript">
    $('.btn_click_load').click(function() {  //click to load comments
        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document;
            var s = d.createElement('script');
            s.src = '//http-miccall-tech.disqus.com/embed.js';
            s.setAttribute('data-timestamp', + new Date());
            (d.head || d.body).appendChild(s);
        })();
        $('.btn_click_load').css('display','none');
    });
</script>
</div>
<style>
    #disqus-comment{
        background-color: #eee;
        padding: 2pc;
    }
</style>


        </div>
        <!-- Copyright 版权 start -->
                <div id="copyright">
            <ul>
                <li>&copy;Powered By <a target="_blank" rel="noopener" href="https://hexo.io/zh-cn/" style="border-bottom: none;">hexo</a></li>
                <li>Design: <a target="_blank" rel="noopener" href="http://miccall.tech " style="border-bottom: none;">miccall</a></li>
            </ul>
            
                <span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
			
        </div>
    </div>
</body>



 	
</html>
